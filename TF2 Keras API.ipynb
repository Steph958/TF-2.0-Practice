{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF2 Keras API.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOk+Pgn/2MQaZyQRS6l1lR3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AolY1mrTejYe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595772924282,"user_tz":-480,"elapsed":2295,"user":{"displayName":"李澔","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZKpGHBcU2juEjpNnAR0b18ha1We7U--A-PpndZGw=s64","userId":"06227313956344060006"}}},"source":["import tensorflow as tf"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXatZBZ0ZNOQ","colab_type":"text"},"source":["**tf.keras的api**\n","\n","首先，tf.keras與我們所熟知的keras是不太一樣的，以前我們所使用的keras，他的backend其實也是用tensorflow，且Keras有自己做好的模型wrapper。\n","\n","而現在TF2.0 大力將keras整進去，一般來說，在tf.keras下，比較常使用的像是 tf.keras.layer , tf.keras.losses,tf.keras.mertrics,tf.keras.optimzer 等等"]},{"cell_type":"markdown","metadata":{"id":"pbQ29kTlZc3u","colab_type":"text"},"source":["**tf.keras.metrics:**\n","\n","主要像是用在training model的過程，常會用以紀錄像是loss或者accuracy等等的模型數值。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LYYnNW0lZp-E","colab_type":"text"},"source":["假如要建立一些模型衡量指標:"]},{"cell_type":"code","metadata":{"id":"kdRJq7KCYbcd","colab_type":"code","colab":{}},"source":["Model_acc = tf.keras.metrics.Accurcy()\n","#or\n","Model_mean = tf.keras.metrics.Mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXMR3UKcZk8T","colab_type":"text"},"source":["當我們要update或者說增加裡面的數值:"]},{"cell_type":"code","metadata":{"id":"YEQ3t3VwZoPy","colab_type":"code","colab":{}},"source":["Model_acc.update_state(y,pred)\n","#or\n","Model_mean.update_state(current_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCqrrdvSZuCM","colab_type":"text"},"source":["在每一次迭代(iteration)或每一回合(epoch)，在模型裡面print出這些數值:\n","\n"]},{"cell_type":"code","metadata":{"id":"2cljXn7NZxSk","colab_type":"code","colab":{}},"source":["#training steps\n","print(step,'Train_loss:',Model_mean.result().numpy(),\n","      'Train_Acc',Model_acc.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1kCPZkXZ1l8","colab_type":"text"},"source":["當每個epoch跑完要清楚裡面的數值時:"]},{"cell_type":"code","metadata":{"id":"WGHU4HpYZ4mD","colab_type":"code","colab":{}},"source":["Model_mean.reset_states()\n","Model_acc.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gu7FSTfmc-0K","colab_type":"text"},"source":["tf.keras也有提供資料前處理的API:tf.keras.preoprocessing，包含特徵工程以及資料增強等等的前處理方法，可應用於文字辨識、影音辨識等多種情境\n","\n","以下是一個常見的資料增強(data Augmentation)的方法:Random Shift"]},{"cell_type":"code","metadata":{"id":"4nE9_a_1dvIq","colab_type":"code","colab":{}},"source":["(x,y),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","img_shifted = tf.keras.preoprocessing.image.random_shift(\n","    x,\n","    wrg = 0.2,\n","    hrg = 0.2,\n","    fill_mode = 'constant'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1IiEf0hZ5tz","colab_type":"text"},"source":["**2.Model Define：**\n","\n","在Model定義裡像是我們會使用tf.keras.Sequential，它是一個序列模型，可以讓我們輕易地透過這個API來堆疊網路並建立序列化模型。\n","\n","在各層的定義就會使用tf.keras.layers來定義每一層 (Ex: neuron數、activation function等等)。\n","\n","而若是想看一下參數的化，model.trainable_variables可以拿取model中要train的variable (Ex: weight , bias等)。\n","\n","最後就丟進去compile即可！"]},{"cell_type":"markdown","metadata":{"id":"-Tv_6Kblfw8R","colab_type":"text"},"source":["*補充:*\n","\n","*tf.keras還有一個定義模型的方式:* **Functional API**\n","\n","*序列化模型的一個缺點是每一層僅能做到單輸入單輸出，無法多輸入多輸出；想要建立一個較為複雜的模型的話，就要使用Functional API*\n","\n"]},{"cell_type":"code","metadata":{"id":"m8cS9wc7aBAl","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(256,activation=tf.nn.relu),\n","    tf.keras.layers.Dense(128,activation=tf.nn.relu),\n","    tf.keras.layers.Dense(64,activation=tf.nn.relu),\n","    tf.keras.layers.Dense(32,activation=tf.nn.relu),\n","    tf.keras.layers.Dense(16,activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10,activation=tf.nn.relu)\n","])\n","\n","model.build(input_shpae=[-1,28*28])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guyM0q3RbeMl","colab_type":"text"},"source":["想要custom layer:\n","\n","在class中主要就是要實現init、call跟build的方法，然後記得要繼承tf.keras.layer。\n","\n","​ init : 主要就是初始化，以及繼承\n","\n","​ call:執行向前傳導\n","\n","​ build: 輸入shape，定義viarble的等等"]},{"cell_type":"code","metadata":{"id":"ifCyKG_hbjwl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":474},"executionInfo":{"status":"ok","timestamp":1595772929591,"user_tz":-480,"elapsed":815,"user":{"displayName":"李澔","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZKpGHBcU2juEjpNnAR0b18ha1We7U--A-PpndZGw=s64","userId":"06227313956344060006"}},"outputId":"16622e04-e26f-4aab-d1d9-c1bd6c420d76"},"source":["#定義類別\n","class MyDenseLayer(tf.keras.layers.Layer):\n","  def __init__(self, num_outputs):\n","    super(MyDenseLayer, self).__init__()\n","    self.num_outputs = num_outputs\n","\n","  def build(self, input_shape):\n","    self.kernel = self.add_variable(\"kernel\",\n","                                    shape=[int(input_shape[-1]),\n","                                           self.num_outputs])\n","\n","  def call(self, input):\n","    return tf.matmul(input, self.kernel)\n","\n","#呼叫類別中的物件/方法  \n","layer = MyDenseLayer(10)\n","print(layer(tf.zeros([10, 5])))\n","print(layer.trainable_variables)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-b1dc67d06302>:10: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","tf.Tensor(\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n","[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n","array([[-0.18886581, -0.24676901,  0.5485528 ,  0.14527285,  0.35803646,\n","         0.4273072 ,  0.39134473,  0.48106366,  0.35301638,  0.2746222 ],\n","       [ 0.03609514, -0.47452742,  0.6233738 ,  0.5700552 , -0.29246294,\n","        -0.62321186, -0.5627171 , -0.43030277, -0.08164567, -0.61876553],\n","       [ 0.11935896, -0.11348706, -0.57339066,  0.12509286, -0.4366442 ,\n","         0.45993334, -0.28063262, -0.20672333,  0.38035625, -0.38883123],\n","       [ 0.10426813,  0.08950424, -0.56765884,  0.33956206, -0.24978992,\n","         0.41526157, -0.24697891, -0.06439358,  0.07632101, -0.37561396],\n","       [-0.30688214,  0.58520776,  0.29449964, -0.6122563 , -0.25412494,\n","        -0.535735  , -0.45824987, -0.4601091 , -0.07585269, -0.21676227]],\n","      dtype=float32)>]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4LG-Dkjyg5K6","colab_type":"text"},"source":["而自定義一個網路架構也是非常類似的，但他所繼承的類別就是tf.keras.Model"]},{"cell_type":"code","metadata":{"id":"BIGcVs0ZeiLl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"ok","timestamp":1595773543099,"user_tz":-480,"elapsed":1103,"user":{"displayName":"李澔","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZKpGHBcU2juEjpNnAR0b18ha1We7U--A-PpndZGw=s64","userId":"06227313956344060006"}},"outputId":"ee7c4636-3022-451f-afb5-0998cf04d570"},"source":["#定義類別\n","class ResnetIdentityBlock(tf.keras.Model):\n","  def __init__(self, kernel_size, filters):\n","    super(ResnetIdentityBlock, self).__init__(name='')\n","    filters1, filters2, filters3 = filters\n"," \n","    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n","    self.bn2a = tf.keras.layers.BatchNormalization()\n"," \n","    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n","    self.bn2b = tf.keras.layers.BatchNormalization()\n"," \n","    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n","    self.bn2c = tf.keras.layers.BatchNormalization()\n"," \n","  def call(self, input_tensor, training=False):\n","    x = self.conv2a(input_tensor)\n","    x = self.bn2a(x, training=training)\n","    x = tf.nn.relu(x)\n"," \n","    x = self.conv2b(x)\n","    x = self.bn2b(x, training=training)\n","    x = tf.nn.relu(x)\n"," \n","    x = self.conv2c(x)\n","    x = self.bn2c(x, training=training)\n"," \n","    x += input_tensor\n","    return tf.nn.relu(x)\n"," \n","#呼叫類別中的物件/方法   \n","block = ResnetIdentityBlock(1, [1, 2, 3])\n","print(block(tf.zeros([1, 2, 3, 3])))\n","print([x.name for x in block.variables])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[[[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n","['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization/gamma:0', 'resnet_identity_block/batch_normalization/beta:0', 'resnet_identity_block/batch_normalization/moving_mean:0', 'resnet_identity_block/batch_normalization/moving_variance:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_1/gamma:0', 'resnet_identity_block/batch_normalization_1/beta:0', 'resnet_identity_block/batch_normalization_1/moving_mean:0', 'resnet_identity_block/batch_normalization_1/moving_variance:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_2/gamma:0', 'resnet_identity_block/batch_normalization_2/beta:0', 'resnet_identity_block/batch_normalization_2/moving_mean:0', 'resnet_identity_block/batch_normalization_2/moving_variance:0']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tyMg2n-xhInx","colab_type":"text"},"source":["3.Model training\n","\n","一般來說，Model training的api會分為四類：\n","\n","model.compile\n","\n","model.fit\n","\n","model.evaluate\n","\n","model.predict\n","\n","這四項。其實，這些api也很直觀，就是整個ML training的pipeline。\n","\n","若不用tf.keras的api，就像之前Lab一樣，要自己寫tf.GradientTape()來訓練參數。"]},{"cell_type":"code","metadata":{"id":"sr7zwZYNhcs_","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam',\n","                loss='mean_squared_error',\n","                metrics=['mean_squared_error'])\n","\n","history = model.fit(X_train.values ,y_train.values, epochs=100, validation_split = 0.1)\n","\n","model.evaluate(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b30Ag2G3hrWh","colab_type":"text"},"source":["在訓練過程中，我們經常使用Callback函數來監控模型狀況；其中又屬Earlystopping最常使用\n","\n","一般來說會使用驗證資料集的loss和acc來做Early Stopping，主要參數為:\n","\n","min_delta: 改善的幅度門檻\n","\n","patience: 在多少個epoch內若驗證集的改善程度未達設定之門檻即停止訓練\n","\n","mode: 當訓練狀況停止下降為'min'；反之為'max'；自動選擇則是'auto'\n","\n"]},{"cell_type":"code","metadata":{"id":"J4l2BrLAi9_3","colab_type":"code","colab":{}},"source":["tf.keras.callbacks.EarlyStopping(\n","    monitor = 'val_loss', min_delta = 0, patience = 0, mode = 'auto')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HbLlB01jjXt0","colab_type":"text"},"source":["4.Save / Load Model:\n","\n","一般來說，跑完model，我們會希望能把model 儲存下來，或者說跑到一半中斷想要save model，之後可以直接Load model繼續train。\n","\n","儲存的方式有兩種 save/load weight 、save /load model 。\n","\n","Save weight的方式是比較輕量級的方法，但是模型架構等等都要先定義好且相同。\n","\n","Save model就是直接全部存下來，不需要先定義模型架構。"]},{"cell_type":"code","metadata":{"id":"3G9EhrZ-jng-","colab_type":"code","colab":{}},"source":["#Save weight\n","model.save_weights('weights.ckpt')\n","\n","#Load weight\n","model = create_model()\n","model.load_weights('weights.ckpt')\n","\n","# save model\n","model.save('model.h5')\n","\n","# load model\n","model = tf.keras.models.load_model('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5FaztkCj2Be","colab_type":"text"},"source":["另外，如果我們在訓練過程中想直接儲存權重的話，也可以直接使用ModelCheckpoint來儲存模型每一個epoch的權重\n","\n","使用該函數的好處在於這個API可將模型使用的權重紀錄下來，當模型因為訓練過久或是系統不穩定而中斷訓練時，可透過記錄繼續訓練"]},{"cell_type":"code","metadata":{"id":"VZWvlLXekwyM","colab_type":"code","colab":{}},"source":["#直接儲存最佳權重之模型\n","checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc',\n","                             verbose = 1, save_best_only = True, \n","                             mode = 'max')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9cNjzldtlKm-","colab_type":"text"},"source":[""]}]}